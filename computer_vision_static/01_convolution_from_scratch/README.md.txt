# Convolution From Scratch

**Domain:** Static-image Computer Vision  
**Project:** `01_convolution_from_scratch`
 **What it does:** Implements basic 2D convolution filters (edgeâ€detection, sharpen, blur) from scratch in NumPy, then trains a tiny CNN to classify synthetic images of circles, rectangles and triangles.
---

## ğŸ“‚ Project Structure

```text
01_convolution_from_scratch/
â”œâ”€â”€ 01_convolution_from_scratch.ipynb   â† Jupyter notebook demo
â”œâ”€â”€ models/                             â† Saved model checkpoints
â”‚   â””â”€â”€ best_shapes_cnn.pt
â”œâ”€â”€ outputs/                            â† Generated figures & previews
â”‚   â”œâ”€â”€ convolution_demo.png
â”‚   â”œâ”€â”€ shapes_preview.png
â”‚   â”œâ”€â”€ qualitative_inspection.png
â”‚   â””â”€â”€ training_curves.png
â””â”€â”€ README.md                           â† You are here!

ğŸ“ Overview
This notebook demonstrates:

Pure-NumPy 2D convolution on a CIFAR-10 sample

Three filter kernels:

Edge detection

Sharpening

Gaussian blur

Saving the filtered figure to
outputs/convolution_demo.png

Previewing random synthetic shapes in
outputs/shapes_preview.png

It then trains a tiny CNN on the synthetic shapes dataset and saves the best model to
models/best_shapes_cnn.pt.

âš™ï¸ Requirements
Run within a Python 3.8+ environment with:


pip install torch torchvision \
            torchmetrics matplotlib \
            scikit-image tqdm psutil
ğŸš€ Usage
Open 01_convolution_from_scratch.ipynb in Jupyter.

Restart & Run All (Kernel â†’ Restart & Run All).

When complete, inspect:

Filtered figure: outputs/convolution_demo.png

Shapes preview: outputs/shapes_preview.png

Qualitative inspection: outputs/qualitative_inspection.png

Training curves: outputs/training_curves.png

Model weights: models/best_shapes_cnn.pt

ğŸ” Next Steps
Try additional kernels (e.g. Sobel, Laplacian)

Extend convolution to colour channels

Compare against scipy.ndimage.convolve or PyTorchâ€™s nn.Conv2d

ğŸ“œ License
MIT License â€” feel free to adapt or fork for your own learning.
